# -*- coding: utf-8 -*-
"""Digital Twin AQC MiniProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tpxpmOQGncMYLlacXsaGc6S5Ij1QHjUe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

# --------- Load and preprocess ---------
data_path = "tool_wear_dataset.csv"
target_col = "Wear_Class"

df = pd.read_csv(data_path)

# Example: label is categorical ("Healthy", "Moderate", "Worn") -> make binary:
if df[target_col].dtype == "object":
    y = (df[target_col] != "Healthy").astype(int)
else:
    y = df[target_col].astype(int)

X = df.drop(columns=[target_col])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

clf = RandomForestClassifier(
    n_estimators=200,
    n_jobs=-1,
    class_weight="balanced",
    random_state=42
)
clf.fit(X_train, y_train)

y_proba_test = clf.predict_proba(X_test)[:, 1]
y_pred = (y_proba_test > 0.5).astype(int)

print("ROC-AUC:", round(roc_auc_score(y_test, y_proba_test), 4))
print(classification_report(y_test, y_pred))

# --------- Cost model and threshold metrics ---------
C_inspect = 0.5   # inspection cost
C_scrap   = 2.0   # cost to scrap/replace worn tool caught internally
C_bad     = 10.0  # cost if worn tool is not detected (bad parts, downtime, etc.)

def metrics_for_threshold(y_true, y_proba, threshold):
    y_true = np.asarray(y_true)
    y_proba = np.asarray(y_proba)

    inspect = (y_proba >= threshold)
    bad = (y_true == 1)
    good = ~bad

    TP = inspect & bad
    FP = inspect & good
    FN = (~inspect) & bad
    TN = (~inspect) & good

    n = len(y_true)
    n_bad = bad.sum()

    inspection_rate = inspect.mean()
    escape_rate = FN.sum() / n
    escape_ratio_bad = FN.sum() / n_bad if n_bad > 0 else np.nan
    scrap_rate = TP.sum() / n

    cost = np.zeros(n, dtype=float)
    cost[TP] = C_inspect + C_scrap
    cost[FP] = C_inspect
    cost[FN] = C_bad

    total_cost = cost.sum()
    avg_cost_per_part = total_cost / n

    return {
        "threshold": threshold,
        "inspection_rate": inspection_rate,
        "escape_rate": escape_rate,
        "escape_ratio_bad": escape_ratio_bad,
        "scrap_rate": scrap_rate,
        "total_cost": total_cost,
        "avg_cost_per_part": avg_cost_per_part
    }

thresholds = np.linspace(0.01, 0.99, 33)
rows = [metrics_for_threshold(y_test, y_proba_test, t) for t in thresholds]
metrics_df = pd.DataFrame(rows).sort_values("threshold").reset_index(drop=True)

best_row = metrics_df.loc[metrics_df["avg_cost_per_part"].idxmin()]
best_threshold = float(best_row["threshold"])
print("\nBest threshold:", round(best_threshold, 3))
print("Min avg cost per part:", round(best_row["avg_cost_per_part"], 4))

# --------- Pareto frontier  ---------
def pareto_front_two_objectives(df, x_col, y_col):
    data = df[[x_col, y_col]].values
    n = data.shape[0]
    is_eff = np.ones(n, dtype=bool)
    for i in range(n):
        if not is_eff[i]:
            continue
        x_i, y_i = data[i]
        dom = (data[:, 0] <= x_i) & (data[:, 1] <= y_i) & (
            (data[:, 0] < x_i) | (data[:, 1] < y_i)
        )
        is_eff[dom] = False
        is_eff[i] = True
    return is_eff

metrics_df["pareto_cost_inspect"] = pareto_front_two_objectives(
    metrics_df, "inspection_rate", "avg_cost_per_part"
)

plt.figure()
plt.scatter(metrics_df["inspection_rate"],
            metrics_df["avg_cost_per_part"], s=30)
pareto_pts = metrics_df[metrics_df["pareto_cost_inspect"]]
plt.scatter(pareto_pts["inspection_rate"],
            pareto_pts["avg_cost_per_part"], s=60)
plt.xlabel("Inspection rate")
plt.ylabel("Average cost per part")
plt.title("Cost vs Inspection Rate")
plt.grid(True)
plt.show()

plt.figure()
plt.scatter(metrics_df["escape_rate"],
            metrics_df["avg_cost_per_part"], s=30)
plt.xlabel("Escape rate")
plt.ylabel("Average cost per part")
plt.title("Cost vs Escape Rate")
plt.grid(True)
plt.show()

# --------- Streaming digital twin simulation (runtime loop) ---------
def simulate_twin(model, X_stream, y_stream, threshold):
    X_stream = X_stream.reset_index(drop=True)
    y_stream = pd.Series(y_stream).reset_index(drop=True)

    n = len(X_stream)
    log = []

    total_cost = 0.0
    for t in range(n):
        x_t = X_stream.iloc[t:t+1]
        y_t = int(y_stream.iloc[t])

        p_fail = model.predict_proba(x_t)[:, 1][0]
        inspect = p_fail >= threshold

        if inspect:
            if y_t == 1:
                step_cost = C_inspect + C_scrap
                outcome = "inspect_and_replace"
            else:
                step_cost = C_inspect
                outcome = "inspect_unnecessary"
        else:
            if y_t == 1:
                step_cost = C_bad
                outcome = "undetected_worn"
            else:
                step_cost = 0.0
                outcome = "normal_operation"

        total_cost += step_cost

        log.append({
            "time_step": t,
            "true_label": y_t,
            "predicted_risk": p_fail,
            "inspect": int(inspect),
            "outcome": outcome,
            "step_cost": step_cost,
            "cum_cost": total_cost
        })

    return pd.DataFrame(log)

sim_df = simulate_twin(clf, X_test, y_test, best_threshold)

print("\nSimulation head:")
print(sim_df.head())

plt.figure()
plt.plot(sim_df["time_step"], sim_df["cum_cost"])
plt.xlabel("Time step")
plt.ylabel("Cumulative cost")
plt.title("Digital Twin Runtime Cost Trajectory")
plt.grid(True)
plt.show()

# ====== Feature importance (Random Forest) ======
import numpy as np
import pandas as pd

feat_imp = pd.Series(clf.feature_importances_, index=X_train.columns)
feat_imp = feat_imp.sort_values(ascending=False)

print("Top 15 individual features:\n")
print(feat_imp.head(15))

# Optional: plot
plt.figure()
feat_imp.head(15).iloc[::-1].plot(kind="barh")
plt.xlabel("Importance")
plt.title("Top 15 Feature Importances (Random Forest)")
plt.tight_layout()
plt.show()

# ====== Sensor-group importance by prefix ======
# Simple grouping: everything before first '_' is treated as sensor group.
groups = {}
for col, imp in feat_imp.items():
    if "_" in col:
        g = col.split("_")[0]
    else:
        g = col[:3]  # fallback: first 3 chars
    groups.setdefault(g, 0.0)
    groups[g] += imp

group_imp = pd.Series(groups).sort_values(ascending=False)

print("\nSensor-group importances:\n")
print(group_imp)

plt.figure()
group_imp.head(10).iloc[::-1].plot(kind="barh")
plt.xlabel("Grouped importance")
plt.title("Top Sensor Groups by Importance")
plt.tight_layout()
plt.show()

# ====== Cost-scenario sensitivity analysis ======

def optimal_threshold_for_costs(y_true, y_proba, C_inspect, C_scrap, C_bad, thresholds):
    def metrics_for_threshold_costs(y_true, y_proba, threshold):
        y_true = np.asarray(y_true)
        y_proba = np.asarray(y_proba)

        inspect = (y_proba >= threshold)
        bad = (y_true == 1)
        good = ~bad

        TP = inspect & bad
        FP = inspect & good
        FN = (~inspect) & bad

        n = len(y_true)

        cost = np.zeros(n, dtype=float)
        cost[TP] = C_inspect + C_scrap
        cost[FP] = C_inspect
        cost[FN] = C_bad

        total_cost = cost.sum()
        avg_cost = total_cost / n
        return avg_cost

    rows = []
    for t in thresholds:
        avg_cost = metrics_for_threshold_costs(y_true, y_proba, t)
        rows.append({"threshold": t, "avg_cost": avg_cost})

    df_cost = pd.DataFrame(rows)
    best_row = df_cost.loc[df_cost["avg_cost"].idxmin()]
    return best_row["threshold"], best_row["avg_cost"], df_cost

# Define a few scenarios
scenarios = [
    {"name": "Base",        "C_inspect": 0.5, "C_scrap": 2.0, "C_bad": 10.0},
    {"name": "CheapCheck",  "C_inspect": 0.2, "C_scrap": 2.0, "C_bad": 10.0},
    {"name": "CostlyCheck", "C_inspect": 1.0, "C_scrap": 2.0, "C_bad": 10.0},
    {"name": "HighPenalty", "C_inspect": 0.5, "C_scrap": 2.0, "C_bad": 20.0},
]

results = []
for sc in scenarios:
    th, cost_val, df_cost = optimal_threshold_for_costs(
        y_test, y_proba_test,
        sc["C_inspect"], sc["C_scrap"], sc["C_bad"],
        thresholds
    )
    results.append({
        "Scenario": sc["name"],
        "C_inspect": sc["C_inspect"],
        "C_scrap": sc["C_scrap"],
        "C_bad": sc["C_bad"],
        "Best_threshold": float(th),
        "Min_avg_cost": float(cost_val)
    })

scenario_df = pd.DataFrame(results)
print("\nOptimal thresholds under different cost scenarios:\n")
print(scenario_df)

# Optional: plot thresholds per scenario
plt.figure()
plt.bar(scenario_df["Scenario"], scenario_df["Best_threshold"])
plt.ylabel("Optimal threshold")
plt.title("Threshold Sensitivity to Cost Parameters")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 3))

ax = plt.gca()
ax.axis("off")

# helper
def box(x, text):
    rect = plt.Rectangle((x, 0.4), 2.5, 0.8,
                         fill=False, linewidth=1.5)
    ax.add_patch(rect)
    ax.text(x + 1.25, 0.8, text,
            ha="center", va="center", fontsize=10)

# draw boxes
box(0.0, "Multi-sensor\nCNC dataset")
box(3.0, "Random Forest\nwear classifier")
box(6.0, "Cost model\n(inspection, scrap, escape)")
box(9.0, "Digital twin\nsimulation\n(runtime cost)")

# arrows
for x in [3, 6, 9]:
    ax.annotate("", xy=(x, 0.8), xytext=(x-0.5, 0.8),
                arrowprops=dict(arrowstyle="->"))

plt.xlim(-0.5, 12)
plt.ylim(0, 2)
plt.title("Digital Twin Method and Pipeline", fontsize=14)
plt.tight_layout()
plt.savefig("pipeline_fixed.png", dpi=300, bbox_inches="tight")
plt.show()

